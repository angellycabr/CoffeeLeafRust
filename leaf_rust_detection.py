# -*- coding: utf-8 -*-
"""EE490 Directed Research

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BPiuCPh4_dhATptgbKkwzGUKjMTcUMWY
"""

from google.colab import drive
import random
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix

from sklearn.model_selection import train_test_split

# Read the dataset into a dataframe.
drive.mount('/content/drive')
data = pd.read_csv('drive/MyDrive/Directed Research/equalized_dataset.csv', header=0)

# Separate the dataframe into feature set and target variable.
X = data.iloc[:,1:]
y = data.iloc[:,0]

# Print the shape of feature set and target variable.
print(X.shape)
print(y.shape)

# Plot the distribution of data per class.
Status = {0: "Healthy", 1: "Leaf Rust"}
y_series = y.apply(lambda x: Status[x])

sample_counts = y_series.value_counts()
print("Number of samples per class:")
print(sample_counts)

plt.figure()
sb.countplot(x=y_series, data=y)
plt.xlabel("Status Label")
plt.ylabel("Number of Samples")
plt.title("Distribution of Samples")
plt.show()

# Plotting 25 random samples.
fig, axes = plt.subplots(4, 4, figsize=(8,8))

samples = data.sample(16).reset_index(drop=True)

for index, value in enumerate(axes.ravel()):
    img = samples.iloc[index, 1:].values.reshape(128, 128)
    value.imshow(img, cmap='gray')
    value.set_title(Status[samples.iloc[index, 0]])
    value.axis('off')
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024, stratify=y)

# Scale the train and test features.
X_train = X_train/255
X_test = X_test/255

"""Method 2: Testing with Simple NN"""

import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(128, 128)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(2, activation='softmax')
])
model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=25)

X_train_reshaped = X_train.values.reshape(-1, 128, 128)
X_test_reshaped = X_test.values.reshape(-1, 128, 128)

# Ensure the pixel values are in the correct format
X_train_reshaped = X_train_reshaped.astype('float32')
X_test_reshaped = X_test_reshaped.astype('float32')

m = model.fit(X_train_reshaped, y_train, epochs=150, batch_size=64, callbacks=[callback], validation_data=(X_test_reshaped, y_test))

pd.DataFrame(m.history).plot()
plt.title("Simple Four Layer Neural Network")
plt.show()

# Display the accuracy of your model.
loss, acc = model.evaluate(X_test_reshaped, y_test)
print("The accuracy:", acc)
print("The loss:", loss)

y_pred = model.predict(X_test_reshaped)
y_pred = np.argmax(y_pred, axis=1)
misclassified_idx = np.where(y_pred != y_test.values)[0][0]

predicted = y_pred[misclassified_idx]
actual = y_test.iloc[misclassified_idx]

plt.imshow(X_test.iloc[misclassified_idx].values.reshape(128, 128), cmap='gray')
plt.title(f"Actual: {Status[actual]} | Predicted: {Status[predicted]}")
plt.axis('off')
plt.show()

"""Method 2 CNN with Pooling and Convolution"""

import tensorflow as tf
from tensorflow import keras

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(128, 128, 1)),
  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D(2, 2),

  tf.keras.layers.BatchNormalization(),

  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=60)

X_train_reshaped = X_train.values.reshape(-1, 128, 128)
X_test_reshaped = X_test.values.reshape(-1, 128, 128)

# Ensure the pixel values are in the correct format
X_train_reshaped = X_train_reshaped.astype('float32')
X_test_reshaped = X_test_reshaped.astype('float32')

c = model.fit(X_train_reshaped, y_train, epochs=200, batch_size=64, callbacks=[callback], validation_data=(X_test_reshaped, y_test))

plt.figure(figsize=(6,4))
plt.plot(c.history['accuracy'], 'black', linewidth=2.0)
plt.plot(c.history['val_accuracy'], 'red', linewidth=2.0)
plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)
plt.xlabel('Epochs', fontsize=10)
plt.ylabel('Accuracy', fontsize=10)
plt.title('Accuracy Curves', fontsize=12)

from sklearn.metrics import f1_score, precision_score, recall_score
y_pred = model.predict(X_test_reshaped)
y_pred = np.argmax(y_pred, axis=1)

def dice_coefficient(y_true, y_pred):
    intersection = np.sum(y_true * y_pred)
    return (2. * intersection) / (np.sum(y_true) + np.sum(y_pred))

# using F1 score over sparse
loss, acc = model.evaluate(X_test_reshaped, y_test)
f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for imbalanced classes
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print("The accuracy:", acc)
print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)
print("Dice Coefficient:", dice_coefficient(y_test, y_pred))

y_pred = model.predict(X_test_reshaped)
y_pred = np.argmax(y_pred, axis=1)
misclassified_idx = np.where(y_pred != y_test.values)[0][0]

predicted = y_pred[misclassified_idx]
actual = y_test.iloc[misclassified_idx]

plt.imshow(X_test.iloc[misclassified_idx].values.reshape(128, 128), cmap='gray')
plt.title(f"Actual: {Status[actual]} | Predicted: {Status[predicted]}")
plt.axis('off')
plt.show()

#!pip install visualkeras

import visualkeras

visualkeras.layered_view(model).show
visualkeras.layered_view(model)

from PIL import ImageFont
visualkeras.layered_view(model, legend=True)  # font is optional!

#  Visualize the predicted and actual image labels for the first 25 images of the test partition (5)
plt.figure(figsize=(10, 12))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_test_reshaped[i].reshape(84, 84), cmap='gray')
    plt.xlabel(f"Actual: {Status[y_test.iloc[i]]}\nPredicted: {Status[y_pred[i]]}")
plt.show()